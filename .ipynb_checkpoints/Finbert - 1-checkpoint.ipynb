{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f43a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swarup\\anaconda3\\envs\\condaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dd921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc95b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "What Are The Disadvantages of Cryptocurrency?\n",
    "Investing in cryptocurrency might look appealing and profitable but investors should also consider a few downsides to it. \n",
    "\n",
    "Cryptocurrency claims to be an anonymous form of transaction, but they are actually pseudonymous which means they leave a digital trail that the Federal Bureau of Investigation can decode. So, there’s a possibility of interference from federal or government authorities to track the financial transactions of normal people.  \n",
    "On a blockchain, there is a constant risk of a 51% attack which means It is a situation when a miner or group of them gets more than 50% of the network’s mining hash rate control. While in control, an ill-natured group can reverse the transaction that is completed, pause the transaction in process, double spend coins, prevent new transactions from getting validation and much more. Nevertheless, this attack is only a risk to recently hard-forked networks and new blockchains.\n",
    "The majority of blockchains work on the proof-of-work consensus mechanism. Network participants are required to use powerful ASIC computers and the right hash to make a block added to the network. Due to this, there is excessive power consumption and countries are taking majors to lower its impact on the environment. \n",
    "The lack of key policies related to transactions serves as a major drawback of cryptocurrencies. The no refund or cancellation policy can be considered the default stance for transactions wrongly made across crypto wallets and each crypto stock exchange or app has its own rules.\n",
    "Are Cryptocurrencies Legal In India?\n",
    "Cryptocurrencies as a payment medium are not regulated or issued by any central authority in India. There are no guidelines laid down for sorting disagreements while dealing with cryptocurrency. So, if you wish to trade in crypto, do it at your own risk. \n",
    "\n",
    "Nirmala Sitharaman, the Finance Minister of India, initiated a tax on digital assets that has increased the discussion on the cryptocurrency legality in the country. \n",
    "\n",
    "Given the stance of the Reserve Bank Of India (RBI) Governor and other key ministers from time to time, it can be safe to state cryptocurrency is not banned in India. Till 2022, cryptocurrency was unregulated in the country. This changed after the government set forth a 30% and 1% tax on profits from cryptocurrencies and tax deducted at source respectively in the Union Budget of 2022. This event marked the Indian government’s official regulation of cryptocurrency in the country. \n",
    "\n",
    "While many supported the decision as it marks the very start of the road to getting cryptocurrency recognition, the Government of India still has to issue an official note for cryptocurrencies to be considered legal in India. \n",
    "\n",
    "Tax on Cryptocurrency in India\n",
    "Tax on cryptocurrency is one of the most confusing investment aspects in India. In the beginning years, there was no income tax or goods and services tax (GST) on cryptocurrencies in India but in the recent Union Budget 2022, a tax regime for digital or virtual assets that include cryptocurrency has been introduced. \n",
    "\n",
    "Crypto investors are required to keep a well-calculated record of losses and gains as a part of their income.\n",
    "On the earnings from the transfer of virtual or digital assets, a 30% tax will be charged. The tax includes cryptocurrencies, NFTs, etc.\n",
    "Cost of acquisition along with no deduction will be permitted while reporting gains from the transfer of virtual or digital assets.\n",
    "A tax of 1% on tax deducted at source (TDS) on the buyer’s payment if it crosses the threshold limit.\n",
    "If someone receives cryptocurrency as a gift or it is transferred then it is subjected to tax at the beneficiary’s end. \n",
    "If investors face any loss from the virtual or digital asset investment, it cannot be recovered against other income.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fe828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2054,  2024,  1996, 20502,  2015,  1997, 19888, 10085,  3126,  7389,\n",
       "          5666,  1029, 19920,  1999, 19888, 10085,  3126,  7389,  5666,  2453,\n",
       "          2298, 16004,  1998, 15282,  2021,  9387,  2323,  2036,  5136,  1037,\n",
       "          2261, 12482,  8621,  2000,  2009,  1012, 19888, 10085,  3126,  7389,\n",
       "          5666,  4447,  2000,  2022,  2019, 10812,  2433,  1997, 12598,  1010,\n",
       "          2021,  2027,  2024,  2941, 13881,  3560,  2029,  2965,  2027,  2681,\n",
       "          1037,  3617,  4446,  2008,  1996,  2976,  4879,  1997,  4812,  2064,\n",
       "         21933,  3207,  1012,  2061,  1010,  2045,  1521,  1055,  1037,  6061,\n",
       "          1997, 11099,  2013,  2976,  2030,  2231,  4614,  2000,  2650,  1996,\n",
       "          3361, 11817,  1997,  3671,  2111,  1012,  2006,  1037,  3796, 24925,\n",
       "          2078,  1010,  2045,  2003,  1037,  5377,  3891,  1997,  1037,  4868,\n",
       "          1003,  2886,  2029,  2965,  2009,  2003,  1037,  3663,  2043,  1037,\n",
       "         18594,  2030,  2177,  1997,  2068,  4152,  2062,  2084,  2753,  1003,\n",
       "          1997,  1996,  2897,  1521,  1055,  5471, 23325,  3446,  2491,  1012,\n",
       "          2096,  1999,  2491,  1010,  2019,  5665,  1011,  3267,  2094,  2177,\n",
       "          2064,  7901,  1996, 12598,  2008,  2003,  2949,  1010,  8724,  1996,\n",
       "         12598,  1999,  2832,  1010,  3313,  5247,  7824,  1010,  4652,  2047,\n",
       "         11817,  2013,  2893, 27354,  1998,  2172,  2062,  1012,  6600,  1010,\n",
       "          2023,  2886,  2003,  2069,  1037,  3891,  2000,  3728,  2524,  1011,\n",
       "          9292,  2098,  6125,  1998,  2047,  3796, 24925,  3619,  1012,  1996,\n",
       "          3484,  1997,  3796, 24925,  3619,  2147,  2006,  1996,  6947,  1011,\n",
       "          1997,  1011,  2147, 10465,  7337,  1012,  2897,  6818,  2024,  3223,\n",
       "          2000,  2224,  3928,  2004,  2594,  7588,  1998,  1996,  2157, 23325,\n",
       "          2000,  2191,  1037,  3796,  2794,  2000,  1996,  2897,  1012,  2349,\n",
       "          2000,  2023,  1010,  2045,  2003, 11664,  2373,  8381,  1998,  3032,\n",
       "          2024,  2635, 15279,  2000,  2896,  2049,  4254,  2006,  1996,  4044,\n",
       "          1012,  1996,  3768,  1997,  3145,  6043,  3141,  2000, 11817,  4240,\n",
       "          2004,  1037,  2350,  4009,  5963,  1997, 19888, 10085,  3126,  7389,\n",
       "          9243,  1012,  1996,  2053, 25416,  8630,  2030, 16990,  3343,  2064,\n",
       "          2022,  2641,  1996, 12398, 11032,  2005, 11817, 29116,  2081,  2408,\n",
       "         19888,  2080, 15882,  2015,  1998,  2169, 19888,  2080,  4518,  3863,\n",
       "          2030, 10439,  2038,  2049,  2219,  3513,  1012,  2024, 19888, 10085,\n",
       "          3126,  7389,  9243,  3423,  1999,  2634,  1029, 19888, 10085,  3126,\n",
       "          7389,  9243,  2004,  1037,  7909,  5396,  2024,  2025, 12222,  2030,\n",
       "          3843,  2011,  2151,  2430,  3691,  1999,  2634,  1012,  2045,  2024,\n",
       "          2053, 11594,  4201,  2091,  2005, 22210, 23145,  2096,  7149,  2007,\n",
       "         19888, 10085,  3126,  7389,  5666,  1012,  2061,  1010,  2065,  2017,\n",
       "          4299,  2000,  3119,  1999, 19888,  2080,  1010,  2079,  2009,  2012,\n",
       "          2115,  2219,  3891,  1012,  9152, 17830,  2721,  4133, 11077,  2386,\n",
       "          1010,  1996,  5446,  2704,  1997,  2634,  1010,  7531,  1037,  4171,\n",
       "          2006,  3617,  7045,  2008,  2038,  3445,  1996,  6594,  2006,  1996,\n",
       "         19888, 10085,  3126,  7389,  5666,  3423,  3012,  1999,  1996,  2406,\n",
       "          1012,  2445,  1996, 11032,  1997,  1996,  3914,  2924,  1997,  2634,\n",
       "          1006, 16929,  1007,  3099,  1998,  2060,  3145,  7767,  2013,  2051,\n",
       "          2000,  2051,  1010,  2009,  2064,  2022,  3647,  2000,  2110, 19888,\n",
       "         10085,  3126,  7389,  5666,  2003,  2025,  7917,  1999,  2634,  1012,\n",
       "          6229, 16798,  2475,  1010, 19888, 10085,  3126,  7389,  5666,  2001,\n",
       "          4895,  2890, 24848,  4383,  1999,  1996,  2406,  1012,  2023,  2904,\n",
       "          2044,  1996,  2231,  2275,  5743,  1037,  2382,  1003,  1998,  1015,\n",
       "          1003,  4171,  2006, 11372,  2013, 19888, 10085,  3126,  7389,  9243,\n",
       "          1998,  4171,  2139, 29510,  2012,  3120,  4414,  1999,  1996,  2586,\n",
       "          5166,  1997, 16798,  2475,  1012,  2023,  2724,  4417,  1996,  2796,\n",
       "          2231,  1521,  1055,  2880,  7816,  1997, 19888, 10085,  3126,  7389,\n",
       "          5666,  1999,  1996,  2406,  1012,  2096,  2116,  3569,  1996,  3247,\n",
       "          2004,  2009,  6017,  1996,  2200,  2707,  1997,  1996,  2346,  2000,\n",
       "          2893, 19888, 10085,  3126,  7389,  5666,  5038,  1010,  1996,  2231,\n",
       "          1997,  2634,  2145,  2038,  2000,  3277,  2019,  2880,  3602,  2005,\n",
       "         19888, 10085,  3126,  7389,  9243,  2000,  2022,  2641,  3423,  1999,\n",
       "          2634,  1012,  4171,  2006, 19888, 10085,  3126,  7389,  5666,  1999,\n",
       "          2634,  4171,  2006, 19888, 10085,  3126,  7389,  5666,  2003,  2028,\n",
       "          1997,  1996,  2087, 16801,  5211,  5919,  1999,  2634,  1012,  1999,\n",
       "          1996,  2927,  2086,  1010,  2045,  2001,  2053,  3318,  4171,  2030,\n",
       "          5350,  1998,  2578,  4171,  1006, 28177,  2102,  1007,  2006, 19888,\n",
       "         10085,  3126,  7389,  9243,  1999,  2634,  2021,  1999,  1996,  3522,\n",
       "          2586,  5166, 16798,  2475,  1010,  1037,  4171,  6939,  2005,  3617,\n",
       "          2030,  7484,  7045,  2008,  2421, 19888, 10085,  3126,  7389,  5666,\n",
       "          2038,  2042,  3107,  1012, 19888,  2080,  9387,  2024,  3223,  2000,\n",
       "          2562,  1037,  2092,  1011, 10174,  2501,  1997,  6409,  1998, 12154,\n",
       "          2004,  1037,  2112,  1997,  2037,  3318,  1012,  2006,  1996, 16565,\n",
       "          2013,  1996,  4651,  1997,  7484,  2030,  3617,  7045,  1010,  1037,\n",
       "          2382,  1003,  4171,  2097,  2022,  5338,  1012,  1996,  4171,  2950,\n",
       "         19888, 10085,  3126,  7389,  9243,  1010,  1050,  6199,  2015,  1010,\n",
       "          4385,  1012,  3465,  1997,  7654,  2247,  2007,  2053,  2139, 16256,\n",
       "          2097,  2022,  7936,  2096,  7316, 12154,  2013,  1996,  4651,  1997,\n",
       "          7484,  2030,  3617,  7045,  1012,  1037,  4171,  1997,  1015,  1003,\n",
       "          2006,  4171,  2139, 29510,  2012,  3120,  1006, 14595,  2015,  1007,\n",
       "          2006,  1996, 17634,  1521,  1055,  7909,  2065,  2009,  7821,  1996,\n",
       "         11207,  5787,  1012,  2065,  2619,  8267, 19888, 10085,  3126,  7389,\n",
       "          5666,  2004,  1037,  5592,  2030,  2009,  2003,  4015,  2059,  2009,\n",
       "          2003, 13532,  2000,  4171,  2012,  1996,  3841, 12879, 24108,  2854,\n",
       "          1521,  1055,  2203,  1012,  2065,  9387,  2227,  2151,  3279,  2013,\n",
       "          1996,  7484,  2030,  3617, 11412,  5211,  1010,  2009,  3685,  2022,\n",
       "          6757,  2114,  2060,  3318,  1012]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "print(len(tokens))\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2398d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59380d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_chunks = tokens['input_ids'][0].split(510)\n",
    "attention_mask_chunks = tokens['attention_mask'][0].split(510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e3a898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_mask_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb1c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_mask_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "928e5bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([510])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_chunks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b241affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ids_and_attention_mask_chunk():\n",
    "    \"\"\"\n",
    "    This function splits the input_ids and attention_mask into chunks of size 'chunksize'. \n",
    "    It also adds special tokens (101 for [CLS] and 102 for [SEP]) at the start and end of each chunk.\n",
    "    If the length of a chunk is less than 'chunksize', it pads the chunk with zeros at the end.\n",
    "    \n",
    "    Returns:\n",
    "        input_id_chunks (List[torch.Tensor]): List of chunked input_ids.\n",
    "        attention_mask_chunks (List[torch.Tensor]): List of chunked attention_masks.\n",
    "    \"\"\"\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "    \n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        \n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        \n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "        \n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            \n",
    "    return input_id_chunks, attention_mask_chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54de248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b04e8d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054, 2024,  ..., 1996, 2586,  102],\n",
       "         [ 101, 5166, 1997,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.stack(input_id_chunks)\n",
    "attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "input_dict = {\n",
    "    'input_ids' : input_ids.long(),\n",
    "    'attention_mask' : attention_mask.int()\n",
    "}\n",
    "\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b59b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4a46d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c1d559b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0269, 0.2417, 0.7314], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**input_dict)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs[0], dim = -1 )\n",
    "\n",
    "mean_probabilities = probabilities.mean(dim = 0)\n",
    "\n",
    "mean_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75395e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mean_probabilities).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5fb217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
